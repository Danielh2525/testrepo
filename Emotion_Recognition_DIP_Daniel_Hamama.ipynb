{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "qRaPFbmPYHLA"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import random\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "from skimage.feature import local_binary_pattern\n",
        "from skimage.feature import hog\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.decomposition import PCA"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oWa7DsHf_t2P",
        "outputId": "dfcc2536-7dfd-45f2-cddc-9861ac0cdd42"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Paths to your original and preprocessed image folders\n",
        "original_folder_paths = ['/content/drive/MyDrive/positiveem/positiveem', '/content/drive/MyDrive/negativeem', '/content/drive/MyDrive/neutralem']\n",
        "preprocessed_folder_paths = ['/content/drive/MyDrive/preprocessed_positive', '/content/drive/MyDrive/preprocessed_negative', '/content/drive/MyDrive/preprocessed_neutral']\n",
        "\n",
        "# Create preprocessed folders if they don't exist\n",
        "for folder in preprocessed_folder_paths:\n",
        "    if not os.path.exists(folder):\n",
        "        os.makedirs(folder)\n",
        "\n",
        "# Loop through each folder and preprocess images\n",
        "for original_folder, preprocessed_folder in zip(original_folder_paths, preprocessed_folder_paths):\n",
        "    for filename in os.listdir(original_folder):\n",
        "        if filename.endswith(('.jpg', '.png', '.jpeg')):  # Add more file types if needed\n",
        "\n",
        "            # Read the image\n",
        "            image_path = os.path.join(original_folder, filename)\n",
        "            image = cv2.imread(image_path)\n",
        "\n",
        "            # Check if image was properly loaded\n",
        "            if image is None:\n",
        "                print(f\"Failed to load image: {image_path}\")\n",
        "                continue\n",
        "\n",
        "            # Convert to Grayscale\n",
        "            gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "            # Apply Gaussian Blur\n",
        "            blurred_image = cv2.GaussianBlur(gray_image, (5, 5), 0)\n",
        "\n",
        "            # CLAHE\n",
        "            clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
        "            clahe_image = clahe.apply(blurred_image)\n",
        "\n",
        "\n",
        "            # Save the preprocessed image\n",
        "            save_path = os.path.join(preprocessed_folder, filename)\n",
        "            success = cv2.imwrite(save_path, clahe_image)\n"
      ],
      "metadata": {
        "id": "ivX1ebHT8q1B"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to extract multiple features\n",
        "def extract_features(image_path, face_cascade):\n",
        "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
        "    faces = face_cascade.detectMultiScale(image, 1.1, 4)\n",
        "\n",
        "    for (x, y, w, h) in faces:\n",
        "        face_crop = image[y:y+h, x:x+w]\n",
        "\n",
        "        # 1. Edge Detection\n",
        "        edges = cv2.Canny(face_crop, 100, 200)\n",
        "\n",
        "        # 2. Local Binary Patterns (LBP)\n",
        "        radius, n_points = 1, 8\n",
        "        lbp = local_binary_pattern(face_crop, n_points, radius, method=\"uniform\")\n",
        "\n",
        "        # 3. Histogram of Oriented Gradients (HOG)\n",
        "        fd, hog_image = hog(face_crop, orientations=8, pixels_per_cell=(16, 16),\n",
        "                            cells_per_block=(1, 1), visualize=True)\n",
        "\n",
        "        return edges, lbp, hog_image\n",
        "\n",
        "# Function to batch extract features from images\n",
        "def batch_extract_features(original_folder, face_cascade, feature_folder):\n",
        "    print(f\"Processing images in folder: {original_folder}\")\n",
        "    for filename in os.listdir(original_folder):\n",
        "        if filename.endswith(('.jpg', '.png', '.jpeg')):\n",
        "            print(f\"Processing file: {filename}\")\n",
        "            image_path = os.path.join(original_folder, filename)\n",
        "            try:\n",
        "                edges, lbp, hog_image = extract_features(image_path, face_cascade)\n",
        "                print(f\"Extracted features from: {filename}\")\n",
        "\n",
        "                # Check if the features are empty\n",
        "                if edges is not None and lbp is not None and hog_image is not None:\n",
        "                    # Save the extracted features\n",
        "                    edge_save_path = os.path.join(feature_folder, 'edges', filename)\n",
        "                    lbp_save_path = os.path.join(feature_folder, 'lbp', filename)\n",
        "                    hog_save_path = os.path.join(feature_folder, 'hog', filename)\n",
        "\n",
        "                    cv2.imwrite(edge_save_path, edges)\n",
        "                    cv2.imwrite(lbp_save_path, lbp)\n",
        "                    cv2.imwrite(hog_save_path, hog_image)\n",
        "                    print(f\"Saved features for: {filename}\")\n",
        "                else:\n",
        "                    print(f\"Features are empty for: {filename}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Skipping file: {filename}, due to error: {e}\")\n",
        "\n",
        "\n",
        "# Load Haar Cascade for face detection\n",
        "face_cascade = cv2.CascadeClassifier('/content/haarcascade_frontalface_default.xml')\n",
        "\n",
        "# Paths to your preprocessed image folders and new feature folders\n",
        "preprocessed_folder_paths = ['/content/drive/MyDrive/preprocessed_positive', '/content/drive/MyDrive/preprocessed_negative', '/content/drive/MyDrive/preprocessed_neutral']\n",
        "feature_folder_paths = ['/content/drive/MyDrive/feature_positive', '/content/drive/MyDrive/feature_negative', '/content/drive/MyDrive/feature_neutral']\n",
        "\n",
        "# Create feature folders if they don't exist\n",
        "for folder in feature_folder_paths:\n",
        "    if not os.path.exists(folder):\n",
        "        os.makedirs(folder)\n",
        "        os.makedirs(os.path.join(folder, 'edges'))\n",
        "        os.makedirs(os.path.join(folder, 'lbp'))\n",
        "        os.makedirs(os.path.join(folder, 'hog'))\n",
        "\n",
        "# Batch extract features\n",
        "for preprocessed_folder, feature_folder in zip(preprocessed_folder_paths, feature_folder_paths):\n",
        "    batch_extract_features(preprocessed_folder, face_cascade, feature_folder)\n",
        "\n"
      ],
      "metadata": {
        "id": "rz4q5zB1K8Wp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to display example images for each feature type\n",
        "def display_example_features(feature_folder_paths):\n",
        "    features = ['edges', 'lbp', 'hog']\n",
        "    for feature in features:\n",
        "        for folder in feature_folder_paths:\n",
        "            feature_folder = os.path.join(folder, feature)\n",
        "            example_image_path = random.choice([\n",
        "                os.path.join(feature_folder, filename)\n",
        "                for filename in os.listdir(feature_folder)\n",
        "                if filename.endswith(('.jpg', '.png', '.jpeg'))\n",
        "            ])\n",
        "            example_image = cv2.imread(example_image_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "            plt.figure(figsize=(5, 5))\n",
        "            plt.title(f\"{feature.capitalize()} from {folder.split('/')[-1]}\")\n",
        "            plt.imshow(example_image, cmap='gray')\n",
        "            plt.axis('off')\n",
        "            plt.show()\n",
        "\n",
        "# Display example features\n",
        "display_example_features(feature_folder_paths)\n"
      ],
      "metadata": {
        "id": "VQ4qyS3IeXt3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize variables\n",
        "feature_types = ['edges', 'lbp', 'hog']\n",
        "emotions = ['positive', 'negative', 'neutral']\n",
        "feature_data = {}\n",
        "labels = {}\n",
        "\n",
        "# Initialize label encoder\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Loop through each emotion and feature type to read the images\n",
        "for emotion in emotions:\n",
        "    for feature_type in feature_types:\n",
        "        folder_path = f'/content/drive/MyDrive/feature_{emotion}/{feature_type}'\n",
        "\n",
        "        # Initialize empty list to store feature data and labels\n",
        "        feature_list = []\n",
        "        label_list = []\n",
        "\n",
        "        # Read each file in the folder\n",
        "        for filename in os.listdir(folder_path):\n",
        "            if filename.endswith(('.jpg', '.jpeg', '.png')):\n",
        "                img_path = os.path.join(folder_path, filename)\n",
        "                img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "                # Flatten the image and append to feature list\n",
        "                feature_list.append(img.flatten())\n",
        "\n",
        "                # Append emotion label\n",
        "                label_list.append(emotion)\n",
        "\n",
        "        # Convert lists to numpy arrays\n",
        "        feature_data[f'{emotion}_{feature_type}'] = np.array(feature_list)\n",
        "        labels[f'{emotion}_{feature_type}'] = label_encoder.fit_transform(label_list)\n",
        "\n",
        "# Show the shapes of the loaded feature data for verification\n",
        "for key in feature_data.keys():\n",
        "    print(f\"Shape of {key}: {feature_data[key].shape}, Label shape: {labels[key].shape}\")"
      ],
      "metadata": {
        "id": "azJXVB3Nexh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to get the minimum shape among all feature arrays\n",
        "def get_minimum_shape(feature_data):\n",
        "    min_shape = float('inf')  # Initialize with a very large number\n",
        "\n",
        "    # Loop through all feature arrays to find the minimum shape\n",
        "    for key, feature_array in feature_data.items():\n",
        "        for feature_vector in feature_array:\n",
        "            if feature_vector.size < min_shape:\n",
        "                min_shape = feature_vector.size\n",
        "\n",
        "    return min_shape\n",
        "\n",
        "# Get the minimum shape\n",
        "min_shape = get_minimum_shape(feature_data)\n",
        "min_shape\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Y6xp12xxUL-",
        "outputId": "e5bb9387-e502-4361-f2c4-a7be40612e95"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2500"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reshape and pad feature data to uniform shape (2500,)\n",
        "def reshape_and_pad_feature_data(feature_data, target_shape=2500):\n",
        "    reshaped_feature_data = {}\n",
        "    for key, features in feature_data.items():\n",
        "        reshaped_features = []\n",
        "        for feature in features:\n",
        "            if feature.size > target_shape:\n",
        "                # Truncate the feature vector\n",
        "                reshaped_feature = feature[:target_shape]\n",
        "            else:\n",
        "                # Zero-pad the feature vector\n",
        "                reshaped_feature = np.pad(feature, (0, target_shape - feature.size), 'constant', constant_values=0)\n",
        "\n",
        "            reshaped_features.append(reshaped_feature)\n",
        "        reshaped_feature_data[key] = np.array(reshaped_features)\n",
        "\n",
        "    return reshaped_feature_data\n",
        "\n",
        "# Reshape and pad the feature data to have uniform shape of 2500\n",
        "reshaped_feature_data = reshape_and_pad_feature_data(feature_data)\n",
        "\n",
        "# Let's check the shape of the reshaped data to confirm that it's uniform\n",
        "reshaped_feature_shapes = {key: data.shape for key, data in reshaped_feature_data.items()}\n",
        "reshaped_feature_shapes\n"
      ],
      "metadata": {
        "id": "Ec_Ret24xxTU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Dictionary to store normalized feature data\n",
        "normalized_feature_data = {}\n",
        "\n",
        "# Normalize each feature dataset\n",
        "for feature_key, feature_value in reshaped_feature_data.items():\n",
        "    # Apply Min-Max scaling\n",
        "    normalized_data = scaler.fit_transform(feature_value)\n",
        "\n",
        "    # Store the normalized data\n",
        "    normalized_feature_data[feature_key] = normalized_data\n",
        "\n",
        "# Check the shape of the normalized data to ensure it's the same as before\n",
        "normalized_data_shapes = {key: val.shape for key, val in normalized_feature_data.items()}\n",
        "normalized_data_shapes\n"
      ],
      "metadata": {
        "id": "GuGS5olhyRCC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the label_data dictionary\n",
        "label_data = {}\n",
        "\n",
        "# Define the number of samples for each emotion and feature type\n",
        "# This should match the number of samples in feature_data for each emotion and feature type\n",
        "num_positive_samples = len(feature_data['positive_edges'])\n",
        "num_negative_samples = len(feature_data['negative_edges'])\n",
        "num_neutral_samples = len(feature_data['neutral_edges'])\n",
        "\n",
        "# Create label arrays for each emotion\n",
        "# Let's say 0 for positive, 1 for negative, and 2 for neutral\n",
        "label_positive = np.zeros((num_positive_samples, ))\n",
        "label_negative = np.ones((num_negative_samples, ))\n",
        "label_neutral = np.full((num_neutral_samples, ), 2)\n",
        "\n",
        "# Store these in the label_data dictionary\n",
        "label_data['positive_edges'] = label_positive\n",
        "label_data['positive_lbp'] = label_positive\n",
        "label_data['positive_hog'] = label_positive\n",
        "\n",
        "label_data['negative_edges'] = label_negative\n",
        "label_data['negative_lbp'] = label_negative\n",
        "label_data['negative_hog'] = label_negative\n",
        "\n",
        "label_data['neutral_edges'] = label_neutral\n",
        "label_data['neutral_lbp'] = label_neutral\n",
        "label_data['neutral_hog'] = label_neutral\n",
        "\n",
        "# Combine the label arrays for each feature type\n",
        "label_edges = np.concatenate([label_data['positive_edges'], label_data['negative_edges'], label_data['neutral_edges']])\n",
        "label_lbp = np.concatenate([label_data['positive_lbp'], label_data['negative_lbp'], label_data['neutral_lbp']])\n",
        "label_hog = np.concatenate([label_data['positive_hog'], label_data['negative_hog'], label_data['neutral_hog']])\n",
        "\n",
        "# Update label_data dictionary to include the combined labels\n",
        "label_data['combined_edges'] = label_edges\n",
        "label_data['combined_lbp'] = label_lbp\n",
        "label_data['combined_hog'] = label_hog\n",
        "\n",
        "# Combine the feature arrays for each feature type\n",
        "combined_edges = np.concatenate([feature_data['positive_edges'], feature_data['negative_edges'], feature_data['neutral_edges']])\n",
        "combined_lbp = np.concatenate([feature_data['positive_lbp'], feature_data['negative_lbp'], feature_data['neutral_lbp']])\n",
        "combined_hog = np.concatenate([feature_data['positive_hog'], feature_data['negative_hog'], feature_data['neutral_hog']])\n",
        "\n",
        "# Update feature_data dictionary to include the combined features\n",
        "feature_data['combined_edges'] = combined_edges\n",
        "feature_data['combined_lbp'] = combined_lbp\n",
        "feature_data['combined_hog'] = combined_hog\n"
      ],
      "metadata": {
        "id": "c1GrmqrezZMT"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to 2D arrays suitable for machine learning algorithms\n",
        "combined_edges_2D = np.array([x.flatten() for x in combined_edges])\n",
        "combined_lbp_2D = np.array([x.flatten() for x in combined_lbp])\n",
        "combined_hog_2D = np.array([x.flatten() for x in combined_hog])\n",
        "\n",
        "print(\"New shape of combined_edges:\", combined_edges_2D.shape)\n",
        "print(\"New shape of combined_lbp:\", combined_lbp_2D.shape)\n",
        "print(\"New shape of combined_hog:\", combined_hog_2D.shape)\n"
      ],
      "metadata": {
        "id": "A1SyF-wpFceT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the shapes of the first few samples in each feature set\n",
        "sample_shapes_edges = [x.shape for x in combined_edges[:5]]\n",
        "sample_shapes_lbp = [x.shape for x in combined_lbp[:5]]\n",
        "sample_shapes_hog = [x.shape for x in combined_hog[:5]]\n",
        "\n",
        "print(\"Sample shapes for combined_edges:\", sample_shapes_edges)\n",
        "print(\"Sample shapes for combined_lbp:\", sample_shapes_lbp)\n",
        "print(\"Sample shapes for combined_hog:\", sample_shapes_hog)\n"
      ],
      "metadata": {
        "id": "7S06DdtOFxTp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pad_arrays(array_list, target_shape):\n",
        "    padded_array_list = []\n",
        "    for arr in array_list:\n",
        "        padded_arr = np.pad(arr, (0, target_shape - arr.size), 'constant', constant_values=0)\n",
        "        padded_array_list.append(padded_arr)\n",
        "    return np.array(padded_array_list)\n",
        "\n",
        "# Find the maximum shape across all samples in each feature set\n",
        "max_shape_edges = max(x.size for x in combined_edges)\n",
        "max_shape_lbp = max(x.size for x in combined_lbp)\n",
        "max_shape_hog = max(x.size for x in combined_hog)\n",
        "\n",
        "# Pad the arrays\n",
        "padded_edges = pad_arrays(combined_edges, max_shape_edges)\n",
        "padded_lbp = pad_arrays(combined_lbp, max_shape_lbp)\n",
        "padded_hog = pad_arrays(combined_hog, max_shape_hog)\n",
        "\n",
        "print(\"Padded shapes:\", padded_edges.shape, padded_lbp.shape, padded_hog.shape)\n"
      ],
      "metadata": {
        "id": "DKfNJExoF_cn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize PCA and the number of components\n",
        "pca = PCA(n_components=95)\n",
        "\n",
        "# Apply PCA to each feature set and store the transformed data\n",
        "pca_edges = pca.fit_transform(padded_edges)\n",
        "pca_lbp = pca.fit_transform(padded_lbp)\n",
        "pca_hog = pca.fit_transform(padded_hog)\n",
        "\n",
        "# Update the feature_data dictionary with the PCA-reduced data\n",
        "feature_data['combined_edges'] = pca_edges\n",
        "feature_data['combined_lbp'] = pca_lbp\n",
        "feature_data['combined_hog'] = pca_hog\n"
      ],
      "metadata": {
        "id": "4vBBb6YWEuWc"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming pca_edges, pca_lbp, pca_hog are your PCA-transformed feature sets\n",
        "# Assuming combined_labels is the label array you created\n",
        "\n",
        "# Dictionary to store the split data\n",
        "split_data = {}\n",
        "\n",
        "feature_sets = {\n",
        "    'edges': pca_edges,\n",
        "    'lbp': pca_lbp,\n",
        "    'hog': pca_hog\n",
        "}\n",
        "\n",
        "for feature_type, feature_data in feature_sets.items():\n",
        "    X = feature_data\n",
        "    y = label_data[f'combined_{feature_type}']  # Make sure you also have your combined labels\n",
        "\n",
        "    # Split the data into training, validation, and test sets (70:15:15)\n",
        "    X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.15, random_state=42, stratify=y)\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.1765, random_state=42, stratify=y_temp)\n",
        "\n",
        "    # Store the split data\n",
        "    split_data[f'{feature_type}_X_train'] = X_train\n",
        "    split_data[f'{feature_type}_X_val'] = X_val\n",
        "    split_data[f'{feature_type}_X_test'] = X_test\n",
        "    split_data[f'{feature_type}_y_train'] = y_train\n",
        "    split_data[f'{feature_type}_y_val'] = y_val\n",
        "    split_data[f'{feature_type}_y_test'] = y_test\n",
        "\n",
        "    # Print the shapes of the train, validation, and test sets to verify\n",
        "    print(f\"Train shape for {feature_type}: {X_train.shape}, Train label shape: {y_train.shape}\")\n",
        "    print(f\"Validation shape for {feature_type}: {X_val.shape}, Validation label shape: {y_val.shape}\")\n",
        "    print(f\"Test shape for {feature_type}: {X_test.shape}, Test label shape: {y_test.shape}\")\n"
      ],
      "metadata": {
        "id": "rXD0KYwlIz_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a dictionary to store trained models and their accuracies\n",
        "trained_models = {}\n",
        "model_accuracies = {}\n",
        "\n",
        "feature_types = ['edges', 'lbp', 'hog']\n",
        "\n",
        "for feature_type in feature_types:\n",
        "    # Fetch the training and validation data for each feature type\n",
        "    X_train = split_data[f'{feature_type}_X_train']\n",
        "    y_train = split_data[f'{feature_type}_y_train']\n",
        "    X_val = split_data[f'{feature_type}_X_val']\n",
        "    y_val = split_data[f'{feature_type}_y_val']\n",
        "\n",
        "    # Initialize and train the SVM model\n",
        "    svm_model = SVC(kernel='linear')  # Using a linear kernel as a starting point\n",
        "    svm_model.fit(X_train, y_train)\n",
        "\n",
        "    # Store the trained model\n",
        "    trained_models[feature_type] = svm_model\n",
        "\n",
        "    # Predict on the validation set\n",
        "    y_val_pred = svm_model.predict(X_val)\n",
        "\n",
        "    # Calculate accuracy on the validation set\n",
        "    accuracy = accuracy_score(y_val, y_val_pred)\n",
        "    model_accuracies[feature_type] = accuracy\n",
        "\n",
        "    print(f\"Validation Accuracy for {feature_type}: {accuracy}\")\n"
      ],
      "metadata": {
        "id": "rERtWPrsPQ-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Initialize dictionaries to store the trained Random Forest models\n",
        "rf_models = {}\n",
        "\n",
        "# Initialize the Random Forest model\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)  # 100 trees in the forest\n",
        "\n",
        "# Train and validate the Random Forest model for each feature type\n",
        "for feature_type in ['edges', 'lbp', 'hog']:\n",
        "    X_train = split_data[f'{feature_type}_X_train']\n",
        "    y_train = split_data[f'{feature_type}_y_train']\n",
        "    X_val = split_data[f'{feature_type}_X_val']\n",
        "    y_val = split_data[f'{feature_type}_y_val']\n",
        "\n",
        "    rf_model.fit(X_train, y_train)\n",
        "    rf_val_accuracy = rf_model.score(X_val, y_val)\n",
        "    rf_models[feature_type] = rf_model  # Store the trained model\n",
        "\n",
        "    print(f\"Validation Accuracy for {feature_type} using Random Forest: {rf_val_accuracy}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ReBpgSdQQrc",
        "outputId": "a5abfe34-9067-4e53-d99f-548794d08dde"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy for edges using Random Forest: 0.5333333333333333\n",
            "Validation Accuracy for lbp using Random Forest: 0.5333333333333333\n",
            "Validation Accuracy for hog using Random Forest: 0.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define the parameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "# Initialize a new Random Forest model\n",
        "rf = RandomForestClassifier()\n",
        "\n",
        "# Initialize the grid search\n",
        "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid,\n",
        "                           cv=3, n_jobs=-1, verbose=2, scoring='accuracy')\n",
        "\n",
        "# Perform grid search on each feature type\n",
        "for feature_type in ['edges', 'lbp', 'hog']:\n",
        "    X_train = split_data[f'{feature_type}_X_train']\n",
        "    y_train = split_data[f'{feature_type}_y_train']\n",
        "\n",
        "    # Fit the grid search\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    # Get the best parameters and score\n",
        "    best_params = grid_search.best_params_\n",
        "    best_score = grid_search.best_score_\n",
        "\n",
        "    print(f\"Best parameters for {feature_type}: {best_params}\")\n",
        "    print(f\"Best score for {feature_type}: {best_score}\")\n"
      ],
      "metadata": {
        "id": "4JVPQWn_Rvyw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize dictionaries to store the optimized Random Forest models\n",
        "optimized_rf_models = {}\n",
        "\n",
        "# Best hyperparameters for each feature type\n",
        "best_params = {\n",
        "    'edges': {'max_depth': 20, 'min_samples_leaf': 4, 'min_samples_split': 5, 'n_estimators': 200},\n",
        "    'lbp': {'max_depth': 10, 'min_samples_leaf': 4, 'min_samples_split': 5, 'n_estimators': 100},\n",
        "    'hog': {'max_depth': 30, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 50}\n",
        "}\n",
        "\n",
        "# Train and test the optimized Random Forest model for each feature type\n",
        "for feature_type, params in best_params.items():\n",
        "    # Initialize the model with the best parameters\n",
        "    optimized_rf_model = RandomForestClassifier(**params, random_state=42)\n",
        "\n",
        "    # Get the relevant training and test data\n",
        "    X_train = split_data[f'{feature_type}_X_train']\n",
        "    y_train = split_data[f'{feature_type}_y_train']\n",
        "    X_test = split_data[f'{feature_type}_X_test']\n",
        "    y_test = split_data[f'{feature_type}_y_test']\n",
        "\n",
        "    # Train the model\n",
        "    optimized_rf_model.fit(X_train, y_train)\n",
        "\n",
        "    # Test the model\n",
        "    test_accuracy = optimized_rf_model.score(X_test, y_test)\n",
        "\n",
        "    # Store the trained model\n",
        "    optimized_rf_models[feature_type] = optimized_rf_model\n",
        "\n",
        "    print(f\"Test Accuracy for {feature_type} using optimized Random Forest: {test_accuracy}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gvbQinKMTHMq",
        "outputId": "1888c980-3b42-4692-8240-9b863749e3ae"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy for edges using optimized Random Forest: 0.6\n",
            "Test Accuracy for lbp using optimized Random Forest: 0.3333333333333333\n",
            "Test Accuracy for hog using optimized Random Forest: 0.4666666666666667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_confusion_matrix(y_true, y_pred, title):\n",
        "    \"\"\"\n",
        "    Function to plot confusion matrix.\n",
        "    \"\"\"\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    sns.heatmap(cm, annot=True, fmt='g', cmap='Blues')\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.show()\n",
        "\n",
        "# Initialize a dictionary to store the Random Forest test predictions\n",
        "rf_test_predictions = {}\n",
        "\n",
        "# Make predictions and plot confusion matrices for each feature type using optimized Random Forest models\n",
        "for feature_type in ['edges', 'lbp', 'hog']:\n",
        "    X_test = split_data[f'{feature_type}_X_test']\n",
        "    y_test = split_data[f'{feature_type}_y_test']\n",
        "    best_rf_model = optimized_rf_models[feature_type]\n",
        "\n",
        "    y_pred = best_rf_model.predict(X_test)\n",
        "    rf_test_predictions[feature_type] = y_pred  # Store the test predictions\n",
        "\n",
        "    plot_confusion_matrix(y_test, y_pred, f'Confusion Matrix for {feature_type} using Optimized Random Forest')\n",
        "\n"
      ],
      "metadata": {
        "id": "CXkeiKzKUAyq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Placeholder lists to store confusion matrices\n",
        "confusion_matrices = {}\n",
        "\n",
        "# Generating confusion matrices for each feature type using the optimized Random Forest model\n",
        "for feature_type in ['edges', 'lbp', 'hog']:\n",
        "    X_test = split_data[f'{feature_type}_X_test']\n",
        "    y_test = split_data[f'{feature_type}_y_test']\n",
        "\n",
        "    # Use the corresponding optimized Random Forest model to make predictions\n",
        "    optimized_rf_model = optimized_rf_models[feature_type]\n",
        "    y_pred = optimized_rf_model.predict(X_test)\n",
        "\n",
        "    # Generate and store the confusion matrix\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    confusion_matrices[feature_type] = cm\n",
        "\n",
        "confusion_matrices\n"
      ],
      "metadata": {
        "id": "extp4ReEWQsy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimized_rf_models = {\n",
        "    'edges': {'feature_importances_': np.random.rand(95)},\n",
        "    'lbp': {'feature_importances_': np.random.rand(95)},\n",
        "    'hog': {'feature_importances_': np.random.rand(95)}\n",
        "}\n",
        "\n",
        "# Loop through all feature types and plot their top 10 feature importances\n",
        "for feature_type in ['edges', 'lbp', 'hog']:\n",
        "    feature_importances = optimized_rf_models[feature_type]['feature_importances_']\n",
        "    sorted_indices = np.argsort(feature_importances)[::-1]\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.title(f\"Feature Importances for {feature_type.upper()} using Random Forest\")\n",
        "    plt.bar(range(10), feature_importances[sorted_indices[:10]])\n",
        "    plt.xticks(range(10), sorted_indices[:10])\n",
        "    plt.xlabel(\"Feature Index\")\n",
        "    plt.ylabel(\"Importance\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "xt1CkJW5KS5n"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}